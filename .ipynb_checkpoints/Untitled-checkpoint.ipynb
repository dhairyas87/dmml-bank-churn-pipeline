{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "035ededa-a01a-402c-9bc3-18bb1cac1f45",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sqlalchemy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataingestion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDataIngestion\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatastorage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDataStorage\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatavalidation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDataValidation\u001b[39;00m\n",
      "File \u001b[0;32m~/DMML_Machine_Learning_Pipeline/dataingestion/DataIngestion.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msqlalchemy\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(\n\u001b[1;32m     13\u001b[0m     level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     handlers\u001b[38;5;241m=\u001b[39m[logging\u001b[38;5;241m.\u001b[39mStreamHandler()]\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sqlalchemy'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import dataingestion.DataIngestion\n",
    "import datastorage.DataStorage\n",
    "import datavalidation.DataValidation\n",
    "import datapreparation.DataPreparation\n",
    "import datatransformationandstorage.DataTransformationAndStorage\n",
    "import featurestore.FeatureStore\n",
    "import dataversioning.DataVersioning\n",
    "import modelbuild.ModelBuild\n",
    "\n",
    "importlib.reload(dataingestion.DataIngestion)\n",
    "importlib.reload(datastorage.DataStorage)\n",
    "importlib.reload(datavalidation.DataValidation)\n",
    "importlib.reload(datapreparation.DataPreparation)\n",
    "importlib.reload(datatransformationandstorage.DataTransformationAndStorage)\n",
    "importlib.reload(featurestore.FeatureStore)\n",
    "importlib.reload(dataversioning.DataVersioning)\n",
    "importlib.reload(modelbuild.ModelBuild)\n",
    "\n",
    "\n",
    "from prefect import task, flow, get_run_logger\n",
    "from dataingestion.DataIngestion import load_csv, load_api, load_db\n",
    "from datastorage.DataStorage import save_csv_or_db, save_api\n",
    "from datavalidation.DataValidation import validate_churn_data\n",
    "from datapreparation.DataPreparation import preprocess_and_eda\n",
    "from datatransformationandstorage.DataTransformationAndStorage import transform_and_store\n",
    "from featurestore.FeatureStore  import create_feature_store, sample_feature_queries\n",
    "from dataversioning.DataVersioning import save_and_version_both\n",
    "from modelbuild.ModelBuild import run_training\n",
    "import sqlite3  \n",
    "\n",
    "@task\n",
    "def ingest_data():\n",
    "    logger = get_run_logger()\n",
    "    csv_url = \"https://synapseaisolutionsa.z13.web.core.windows.net/data/bankcustomerchurn/churn.csv\"\n",
    "    logger.info(f\"📥 Ingesting data from {csv_url}\")\n",
    "    df_csv = load_csv(csv_url, csv_url)\n",
    "    logger.info(f\"✅ Data ingestion complete. Shape: {df_csv.shape}\")\n",
    "    return df_csv\n",
    "\n",
    "@task\n",
    "def store_data(df_csv):\n",
    "    logger = get_run_logger()\n",
    "    base_dir = \"datastorage\"\n",
    "    save_csv_or_db(df_csv, base_dir, \"csv\")\n",
    "    logger.info(f\"✅ Data stored at {base_dir}\")\n",
    "    return base_dir\n",
    "\n",
    "@task\n",
    "def validate_data(df_csv):\n",
    "    logger = get_run_logger()\n",
    "    base_dir = \"datavalidation/reports\"\n",
    "    issues, metadata = validate_churn_data(df_csv, base_dir, \"pdf\")\n",
    "    logger.info(f\"🔍 Validation complete. Issues: {len(issues)} Metadata: {metadata}\")\n",
    "    return issues, metadata\n",
    "\n",
    "@task\n",
    "def prepare_data(df_csv):\n",
    "    logger = get_run_logger()\n",
    "    base_dir = \"datapreparation/prepared\"\n",
    "    df_processed = preprocess_and_eda(df_csv, base_dir)\n",
    "    logger.info(f\"✅ Data preparation complete. Shape: {df_processed.shape}\")\n",
    "    return df_processed\n",
    "\n",
    "@task\n",
    "def transform_data(df_processed):\n",
    "    logger = get_run_logger()\n",
    "    base_dir = \"datatransformationandstorage/transformationandstorage\"\n",
    "    df_txfnstr = transform_and_store(df_processed, base_dir, \"churn\")\n",
    "    logger.info(f\"✅ Data transformation complete. Shape: {df_txfnstr.shape}\")\n",
    "    return df_txfnstr\n",
    "\n",
    "@task\n",
    "def build_feature_store(df_txfnstr):\n",
    "    logger = get_run_logger()\n",
    "    base_path = \"featurestore/featurestore\"\n",
    "    df_feature, conn, db_path = create_feature_store(df_txfnstr, base_path)\n",
    "    sample_feature_queries(conn, base_path)\n",
    "    logger.info(f\"✅ Feature store created at {base_path}, DB path: {db_path}\")\n",
    "    return df_feature, db_path\n",
    "\n",
    "@task\n",
    "def version_data(df_csv, df_feature):\n",
    "    logger = get_run_logger()\n",
    "    save_and_version_both(\n",
    "        df_csv,\n",
    "        df_feature,\n",
    "        \"dataversioning/raw/churn_raw.csv\",\n",
    "        \"dataversioning/transformed/churn_transformed_v1.csv\",\n",
    "        \"churn_raw.csv\",\n",
    "        \"Changes_Commited\"\n",
    "    )\n",
    "    logger.info(\"✅ Data versioning complete.\")\n",
    "\n",
    "@task\n",
    "def train_model(db_path):\n",
    "    logger = get_run_logger()\n",
    "    run_training(db_path)\n",
    "    logger.info(\"✅ Model training complete.\")\n",
    "\n",
    "\n",
    "@flow(name=\"Churn ML Pipeline Orchestration\")\n",
    "def churn_pipeline():\n",
    "    df_csv = ingest_data()\n",
    "    store_data(df_csv)\n",
    "    validate_data(df_csv)\n",
    "    df_processed = prepare_data(df_csv)\n",
    "    df_txfnstr = transform_data(df_processed)\n",
    "    df_feature, db_path = build_feature_store(df_txfnstr)\n",
    "    version_data(df_csv, df_feature)\n",
    "    train_model(db_path)\n",
    "\n",
    "    print(\"✅ Pipeline complete!\")\n",
    "\n",
    "    dot = Digraph(comment=\"Churn ML Pipeline\", format=\"png\")\n",
    "\n",
    "    # Nodes\n",
    "    dot.node(\"ingest_data\", \"📥 Ingest Data\")\n",
    "    dot.node(\"store_data\", \"💾 Store Data\")\n",
    "    dot.node(\"validate_data\", \"🔍 Validate Data\")\n",
    "    dot.node(\"prepare_data\", \"⚙️ Prepare Data\")\n",
    "    dot.node(\"transform_data\", \"🔄 Transform Data\")\n",
    "    dot.node(\"build_feature_store\", \"🏗️ Build Feature Store\")\n",
    "    dot.node(\"version_data\", \"🗂️ Version Data\")\n",
    "    dot.node(\"train_model\", \"🤖 Train Model\")\n",
    "    \n",
    "    # Edges (dependencies)\n",
    "    dot.edge(\"ingest_data\", \"store_data\")\n",
    "    dot.edge(\"ingest_data\", \"validate_data\")\n",
    "    dot.edge(\"ingest_data\", \"prepare_data\")\n",
    "    \n",
    "    dot.edge(\"prepare_data\", \"transform_data\")\n",
    "    dot.edge(\"transform_data\", \"build_feature_store\")\n",
    "    \n",
    "    dot.edge(\"build_feature_store\", \"train_model\")\n",
    "    \n",
    "    dot.edge(\"ingest_data\", \"version_data\")\n",
    "    dot.edge(\"build_feature_store\", \"version_data\")\n",
    "    \n",
    "    # Show graph inline in Jupyter\n",
    "    dot\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    churn_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7df97c6-5a06-4025-a907-4ecaaf3565d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUsage: \u001b[0mprefect [OPTIONS] COMMAND [ARGS]...\n",
      "\u001b[2mTry \u001b[0m\u001b[2;34m'prefect \u001b[0m\u001b[1;2;34m-\u001b[0m\u001b[1;2;34m-help\u001b[0m\u001b[2;34m'\u001b[0m\u001b[2m for help.\u001b[0m\n",
      "\u001b[31m╭─\u001b[0m\u001b[31m Error \u001b[0m\u001b[31m─────────────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m No such command 'orion'.                                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!prefect orion start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793838c6-5393-4a55-b880-42bda744c5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
