{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28b88a5a-fc4d-44d5-a4bf-eba3c109f024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 16:08:26,362 [INFO] CSV data loading from Path with source https://synapseaisolutionsa.z13.web.core.windows.net/data/bankcustomerchurn/churn.csv\n",
      "2025-08-23 16:08:29,563 [INFO] CSV data loaded with shape (10000, 14)\n",
      "2025-08-23 16:08:29,612 [INFO] CSV data stored at datastorage/csv/year=2025/month=08/day=23/csv_DataStorage.csv\n",
      "2025-08-23 16:08:29,612 [INFO] ===== Validation started =====\n",
      "2025-08-23 16:08:29,612 [INFO] Starting check: RowNumber sequence\n",
      "2025-08-23 16:08:29,613 [INFO] Completed check: RowNumber sequence\n",
      "2025-08-23 16:08:29,613 [INFO] Starting check: CustomerId uniqueness\n",
      "2025-08-23 16:08:29,613 [INFO] Completed check: CustomerId uniqueness\n",
      "2025-08-23 16:08:29,614 [INFO] Starting check: CreditScore validity\n",
      "2025-08-23 16:08:29,614 [INFO] Completed check: CreditScore validity\n",
      "2025-08-23 16:08:29,614 [INFO] Starting check: Geography validity\n",
      "2025-08-23 16:08:29,615 [INFO] Completed check: Geography validity\n",
      "2025-08-23 16:08:29,615 [INFO] Starting check: Gender validity\n",
      "2025-08-23 16:08:29,616 [INFO] Completed check: Gender validity\n",
      "2025-08-23 16:08:29,616 [INFO] Starting check: Age range\n",
      "2025-08-23 16:08:29,616 [INFO] Completed check: Age range\n",
      "2025-08-23 16:08:29,617 [INFO] Starting check: Tenure range\n",
      "2025-08-23 16:08:29,617 [INFO] Completed check: Tenure range\n",
      "2025-08-23 16:08:29,617 [INFO] Starting check: Balance non-negative\n",
      "2025-08-23 16:08:29,618 [INFO] Completed check: Balance non-negative\n",
      "2025-08-23 16:08:29,618 [INFO] Starting check: NumOfProducts range\n",
      "2025-08-23 16:08:29,619 [INFO] Completed check: NumOfProducts range\n",
      "2025-08-23 16:08:29,619 [INFO] Starting check: HasCrCard binary values\n",
      "2025-08-23 16:08:29,619 [INFO] Completed check: HasCrCard binary values\n",
      "2025-08-23 16:08:29,620 [INFO] Starting check: IsActiveMember binary values\n",
      "2025-08-23 16:08:29,620 [INFO] Completed check: IsActiveMember binary values\n",
      "2025-08-23 16:08:29,620 [INFO] Starting check: Exited binary values\n",
      "2025-08-23 16:08:29,621 [INFO] Completed check: Exited binary values\n",
      "2025-08-23 16:08:29,621 [INFO] Starting check: EstimatedSalary outliers\n",
      "2025-08-23 16:08:29,623 [WARNING] EstimatedSalary anomalies detected: 10 rows\n",
      "2025-08-23 16:08:29,623 [INFO] Completed check: EstimatedSalary outliers\n",
      "2025-08-23 16:08:29,642 [INFO] Issues report saved at datavalidation/reports/churn_data_issues_20250823_160829.csv\n",
      "2025-08-23 16:08:29,642 [INFO] Metadata report saved at datavalidation/reports/churn_data_metadata_20250823_160829.csv\n",
      "2025-08-23 16:08:29,642 [INFO] PDF report saved at datavalidation/reports/churn_data_report_20250823_160829.pdf\n",
      "2025-08-23 16:08:29,642 [INFO] ===== Validation completed =====\n",
      "2025-08-23 16:08:29,643 [INFO] Starting preprocessing and EDA...\n",
      "2025-08-23 16:08:29,644 [INFO] Handling missing values...\n",
      "2025-08-23 16:08:29,649 [INFO] Encoding categorical string variables...\n",
      "2025-08-23 16:08:29,655 [INFO] Normalizing continuous numeric attributes...\n",
      "2025-08-23 16:08:29,657 [INFO] Generating summary statistics...\n",
      "2025-08-23 16:08:29,668 [INFO] Creating EDA visualizations and saving to PDF...\n",
      "2025-08-23 16:08:30,052 [INFO] Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-08-23 16:08:30,066 [INFO] Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-08-23 16:08:30,120 [INFO] Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-08-23 16:08:30,131 [INFO] Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-08-23 16:08:30,429 [INFO] Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-08-23 16:08:30,434 [INFO] Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-08-23 16:08:30,458 [INFO] Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-08-23 16:08:30,463 [INFO] Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-08-23 16:08:30,713 [INFO] Preprocessing and EDA completed. Cleaned data saved at datapreparation/prepared/cleaned_data_20250823_160830.csv\n",
      "2025-08-23 16:08:30,714 [INFO] EDA PDF report saved at datapreparation/prepared/eda_report_20250823_160829.pdf\n",
      "2025-08-23 16:08:30,714 [INFO] Starting data transformation...\n",
      "2025-08-23 16:08:30,715 [INFO] Dropped columns: ['RowNumber', 'Surname']\n",
      "2025-08-23 16:08:30,715 [INFO] Creating new features...\n",
      "2025-08-23 16:08:30,717 [INFO] Feature engineering completed.\n",
      "2025-08-23 16:08:30,742 [INFO] Data successfully stored in SQLite: datatransformationandstorage/transformationandstorage/churn\n",
      "2025-08-23 16:08:30,743 [INFO] SQL schema design saved at datatransformationandstorage/transformationandstorage/schema_design_20250823_160830.sql\n",
      "2025-08-23 16:08:30,745 [INFO] Sample SQL queries saved at datatransformationandstorage/transformationandstorage/sample_queries.sql\n",
      "2025-08-23 16:08:30,756 [INFO] Query outputs saved at datatransformationandstorage/transformationandstorage/sample_query_outputs.txt\n",
      "2025-08-23 16:08:30,756 [INFO] Transformation summary saved at datatransformationandstorage/transformationandstorage/transformation_summary.txt\n",
      "2025-08-23 16:08:30,757 [INFO] Engineering features for feature store...\n",
      "2025-08-23 16:08:30,788 [INFO] Feature documentation generated at featurestore/featurestore/feature_documentation.md\n",
      "2025-08-23 16:08:30,788 [INFO] Feature store created at featurestore/featurestore/feature_store.db\n",
      "2025-08-23 16:08:30,795 [INFO] Sample queries saved at featurestore/featurestore/sample_queries.txt\n",
      "2025-08-23 16:08:30,796 [INFO] Query results saved at featurestore/featurestore/query_results.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 2180642] Raw dataset update: churn_raw.csv - Initial raw dataset upload\n",
      " 1 file changed, 10001 insertions(+)\n",
      " create mode 100644 dataversioning/raw/churn_raw.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/version_metadata.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# run preprocessing\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[38], line 55\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     sample_feature_queries(conn, base_path)\n\u001b[1;32m     52\u001b[0m     conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 55\u001b[0m     save_and_version_dataset(df_csv, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataversioning/raw/churn_raw.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchurn_raw.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial raw dataset upload\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Transformed dataset from pipeline\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     save_and_version_dataset(df_feature, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataversioning/transformed/churn_transformed_v1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchurn_transformed_v1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplied feature engineering v1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/DMML_Machine_Learning_Pipeline/dataversioning/DataVersioning.py:45\u001b[0m, in \u001b[0;36msave_and_version_dataset\u001b[0;34m(df, path, dataset_name, dataset_type, notes, remote, branch)\u001b[0m\n\u001b[1;32m     35\u001b[0m entry \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset_name,\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotes\u001b[39m\u001b[38;5;124m\"\u001b[39m: notes\n\u001b[1;32m     42\u001b[0m }\n\u001b[1;32m     43\u001b[0m metadata\u001b[38;5;241m.\u001b[39mappend(entry)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(metadata_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     46\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(metadata, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     48\u001b[0m subprocess\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m, metadata_file], check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/version_metadata.json'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import dataingestion.DataIngestion\n",
    "import datastorage.DataStorage\n",
    "import datavalidation.DataValidation\n",
    "import datapreparation.DataPreparation\n",
    "import datatransformationandstorage.DataTransformationAndStorage\n",
    "import featurestore.FeatureStore\n",
    "import dataversioning.DataVersioning\n",
    "\n",
    "importlib.reload(dataingestion.DataIngestion)\n",
    "importlib.reload(datastorage.DataStorage)\n",
    "importlib.reload(datavalidation.DataValidation)\n",
    "importlib.reload(datapreparation.DataPreparation)\n",
    "importlib.reload(datatransformationandstorage.DataTransformationAndStorage)\n",
    "importlib.reload(featurestore.FeatureStore)\n",
    "importlib.reload(dataversioning.DataVersioning)\n",
    "\n",
    "from dataingestion.DataIngestion import load_csv, load_api, load_db\n",
    "from datastorage.DataStorage import save_csv_or_db, save_api\n",
    "from datavalidation.DataValidation import validate_churn_data\n",
    "from datapreparation.DataPreparation import preprocess_and_eda\n",
    "from datatransformationandstorage.DataTransformationAndStorage import transform_and_store\n",
    "from featurestore.FeatureStore  import create_feature_store,sample_feature_queries\n",
    "from dataversioning.DataVersioning import save_and_version_both\n",
    "\n",
    "\n",
    "def main():\n",
    "   \n",
    "\n",
    "    # CSV example\n",
    "    csv_url = \"https://synapseaisolutionsa.z13.web.core.windows.net/data/bankcustomerchurn/churn.csv\"\n",
    "    df_csv = load_csv(csv_url,csv_url)\n",
    "\n",
    "    base_dir = \"datastorage\"\n",
    "    save_csv_or_db(df_csv, base_dir, \"csv\")\n",
    "\n",
    "    base_dir = \"datavalidation/reports\"\n",
    "    issues, metadata = validate_churn_data(df_csv,base_dir,\"pdf\")\n",
    "\n",
    "    base_dir = \"datapreparation/prepared\"\n",
    "    df_processed = preprocess_and_eda(df_csv,base_dir) \n",
    "\n",
    "    base_dir = \"datatransformationandstorage/transformationandstorage\"\n",
    "    df_txfnstr = transform_and_store(df_processed,base_dir,\"churn\")\n",
    "\n",
    "    base_path = \"featurestore/featurestore\"   # <<< define your path here\n",
    "    df_feature,conn, db_path = create_feature_store(df_txfnstr, base_path)\n",
    "\n",
    "    # Run demo queries\n",
    "    sample_feature_queries(conn, base_path)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "    save_and_version_both(df_csv, df_feature,\"dataversioning/raw/churn_raw.csv\", \"dataversioning/transformed/churn_transformed_v1.csv\", \"churn_raw.csv\", \"raw\", \"Initial raw dataset upload\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cac164-549a-4576-8c35-3bbef411696b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde366d-2b45-4c06-99fa-c54613f46171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c53d39-40be-406a-a206-f20604b27412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
