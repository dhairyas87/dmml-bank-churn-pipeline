{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "035ededa-a01a-402c-9bc3-18bb1cac1f45",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'with' statement on line 65 (1117930757.py, line 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 66\u001b[0;36m\u001b[0m\n\u001b[0;31m    f.write(dot.source)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'with' statement on line 65\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import dataingestion.DataIngestion\n",
    "import datastorage.DataStorage\n",
    "import datavalidation.DataValidation\n",
    "import datapreparation.DataPreparation\n",
    "import datatransformationandstorage.DataTransformationAndStorage\n",
    "import featurestore.FeatureStore\n",
    "import dataversioning.DataVersioning\n",
    "import modelbuild.ModelBuild\n",
    "import os\n",
    "importlib.reload(dataingestion.DataIngestion)\n",
    "importlib.reload(datastorage.DataStorage)\n",
    "importlib.reload(datavalidation.DataValidation)\n",
    "importlib.reload(datapreparation.DataPreparation)\n",
    "importlib.reload(datatransformationandstorage.DataTransformationAndStorage)\n",
    "importlib.reload(featurestore.FeatureStore)\n",
    "importlib.reload(dataversioning.DataVersioning)\n",
    "importlib.reload(modelbuild.ModelBuild)\n",
    "\n",
    "\n",
    "from prefect import task, flow, get_run_logger\n",
    "from prefect.tasks import Task\n",
    "from dataingestion.DataIngestion import load_csv, load_api, load_db\n",
    "from datastorage.DataStorage import save_csv_or_db, save_api\n",
    "from datavalidation.DataValidation import validate_churn_data\n",
    "from datapreparation.DataPreparation import preprocess_and_eda\n",
    "from datatransformationandstorage.DataTransformationAndStorage import transform_and_store\n",
    "from featurestore.FeatureStore  import create_feature_store, sample_feature_queries\n",
    "from dataversioning.DataVersioning import save_and_version_both\n",
    "from modelbuild.ModelBuild import run_training\n",
    "import sqlite3  \n",
    "from graphviz import Digraph\n",
    "import os\n",
    "Digraph.format = \"png\"   # ensures inline rendering\n",
    "\n",
    "\n",
    "# Define dependencies between tasks\n",
    "dag_dependencies = {\n",
    "    \"ingest_data\": [\"store_data\", \"validate_data\", \"prepare_data\"],\n",
    "    \"prepare_data\": [\"transform_data\"],\n",
    "    \"transform_data\": [\"build_feature_store\"],\n",
    "    \"build_feature_store\": [\"train_model\", \"version_data\"]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def draw_dag(dependencies, title=\"Churn ML Pipeline\"):\n",
    "    dot = Digraph(comment=title, format=\"png\")\n",
    "\n",
    "    # Add all tasks as nodes\n",
    "    tasks = set(dependencies.keys()) | {t for deps in dependencies.values() for t in deps}\n",
    "    for task in tasks:\n",
    "        dot.node(task, task)\n",
    "\n",
    "    # Add edges\n",
    "    for parent, children in dependencies.items():\n",
    "        for child in children:\n",
    "            dot.edge(parent, child)\n",
    "\n",
    "    output_folder = \"results\"\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
    "    output_file = os.path.join(output_folder, \"orchestor.dot\")\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(dot.source)\n",
    "\n",
    "    return dot\n",
    "    \n",
    "@task\n",
    "def ingest_data():\n",
    "    logger = get_run_logger()\n",
    "    csv_url = \"https://synapseaisolutionsa.z13.web.core.windows.net/data/bankcustomerchurn/churn.csv\"\n",
    "    logger.info(f\"üì• Ingesting data from {csv_url}\")\n",
    "    df_csv = load_csv(csv_url, csv_url)\n",
    "    logger.info(f\"‚úÖ Data ingestion complete. Shape: {df_csv.shape}\")\n",
    "    return df_csv\n",
    "\n",
    "@task\n",
    "def store_data(df_csv):\n",
    "    logger = get_run_logger()\n",
    "    base_dir = \"results/store_data\"\n",
    "    save_csv_or_db(df_csv, base_dir, \"csv\")\n",
    "    logger.info(f\"‚úÖ Data stored at {base_dir}\")\n",
    "    return base_dir\n",
    "\n",
    "@task\n",
    "def validate_data(df_csv):\n",
    "    logger = get_run_logger()\n",
    "    base_dir = \"results/validate_data_reports\"\n",
    "    issues, metadata = validate_churn_data(df_csv, base_dir, \"pdf\")\n",
    "    logger.info(f\"üîç Validation complete. Issues: {len(issues)} Metadata: {metadata}\")\n",
    "    return issues, metadata\n",
    "\n",
    "@task\n",
    "def prepare_data(df_csv):\n",
    "    logger = get_run_logger()\n",
    "    base_dir = \"results/prepared_data\"\n",
    "    df_processed = preprocess_and_eda(df_csv, base_dir)\n",
    "    logger.info(f\"‚úÖ Data preparation complete. Shape: {df_processed.shape}\")\n",
    "    return df_processed\n",
    "\n",
    "@task\n",
    "def transform_data(df_processed):\n",
    "    logger = get_run_logger()\n",
    "    base_dir = \"results/transformation_and_storage\"\n",
    "    df_txfnstr = transform_and_store(df_processed, base_dir, \"churn\")\n",
    "    logger.info(f\"‚úÖ Data transformation complete. Shape: {df_txfnstr.shape}\")\n",
    "    return df_txfnstr\n",
    "\n",
    "@task\n",
    "def build_feature_store(df_txfnstr):\n",
    "    logger = get_run_logger()\n",
    "    base_path = \"results/featurestore\"\n",
    "    df_feature, conn, db_path = create_feature_store(df_txfnstr, base_path)\n",
    "    sample_feature_queries(conn, base_path)\n",
    "    logger.info(f\"‚úÖ Feature store created at {base_path}, DB path: {db_path}\")\n",
    "    return df_feature, db_path\n",
    "\n",
    "@task\n",
    "def version_data(df_csv, df_feature):\n",
    "    logger = get_run_logger()\n",
    "    save_and_version_both(\n",
    "        df_csv,\n",
    "        df_feature,\n",
    "        \"results/dataversion/churn_raw.csv\",\n",
    "        \"results/dataversion/churn_transformed_v1.csv\",\n",
    "        \"churn_raw\",\n",
    "        \"Pipeline_runnning_updates\"\n",
    "    )\n",
    "    logger.info(\"‚úÖ Data versioning complete.\")\n",
    "\n",
    "@task\n",
    "def train_model(db_path):\n",
    "    logger = get_run_logger()\n",
    "    run_training(db_path)\n",
    "    logger.info(\"‚úÖ Model training complete.\")\n",
    "\n",
    "\n",
    "@flow(name=\"Churn ML Pipeline Orchestration\")\n",
    "def churn_pipeline():\n",
    "    df_csv = ingest_data()\n",
    "    store_data(df_csv)\n",
    "    validate_data(df_csv)\n",
    "    df_processed = prepare_data(df_csv)\n",
    "    df_txfnstr = transform_data(df_processed)\n",
    "    df_feature, db_path = build_feature_store(df_txfnstr)\n",
    "    version_data(df_csv, df_feature)\n",
    "    train_model(db_path)\n",
    "    dot = draw_dag(dag_dependencies)\n",
    "    print(\"‚úÖ Pipeline complete!\")\n",
    "\n",
    "# Draw DAG\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    churn_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd442d81-f6a4-41af-845e-fe19a69843b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
